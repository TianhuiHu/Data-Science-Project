{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly Detection is the technique of identifying rare events or observations which can raise suspicions by being statistically different from the rest of the observations. Such “anomalous” behaviour typically translates to some kind of a problem like a credit card fraud, failing machine in a server, a cyber attack, etc.\n",
    "An anomaly can be broadly categorized into three categories --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anormaly detection can be done using the concepts of Machine Learning. It can be done in the following ways –"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised Anomaly Detection**: This method requires a labeled dataset containing both normal and anomalous samples to construct a predictive model to classify future data points. The most commonly used algorithms for this purpose are supervised Neural Networks, Support Vector Machine learning, K-Nearest Neighbors Classifier, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unsupervised Anomaly Detection**: This method does require any training data and instead assumes two things about the data ie. Only a small percentage of data is anomalous and any anomaly is statistically different from the normal samples. Based on the above assumptions, the data is then clustered using a similarity measure and the data points which are far off from the cluster are considered to be anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now demonstrate the process of anomaly detection on a synthetic dataset using the K-Nearest Neighbors algorithm which is included in the pyod module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyod.models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-643e023afde3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfont_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mknn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_outliers_inliers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyod.models'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.utils.data import generate_data, get_outliers_inliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2b210cb6d4bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# generating a random dataset with two features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m X_train, y_train = generate_data(n_train = 300, train_only = True,\n\u001b[0m\u001b[1;32m      3\u001b[0m \t\t\t\t\t\t\t\t\t\t\t\tn_features = 2)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Setting the percentage of outliers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_data' is not defined"
     ]
    }
   ],
   "source": [
    "# generating a random dataset with two features\n",
    "X_train, y_train = generate_data(n_train = 300, train_only = True,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tn_features = 2)\n",
    "\n",
    "# Setting the percentage of outliers\n",
    "outlier_fraction = 0.1\n",
    "\n",
    "# Storing the outliers and inliners in different numpy arrays\n",
    "X_outliers, X_inliers = get_outliers_inliers(X_train, y_train)\n",
    "n_inliers = len(X_inliers)\n",
    "n_outliers = len(X_outliers)\n",
    "\n",
    "# Separating the two features\n",
    "f1 = X_train[:, [0]].reshape(-1, 1)\n",
    "f2 = X_train[:, [1]].reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Visualising the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e991ad5fe0a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# scatter plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Feature 1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Feature 2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f1' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualising the dataset\n",
    "# create a meshgrid\n",
    "xx, yy = np.meshgrid(np.linspace(-10, 10, 200),\n",
    "\t\t\t\t\tnp.linspace(-10, 10, 200))\n",
    "\n",
    "# scatter plot\n",
    "plt.scatter(f1, f2)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Training and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the classifier\n",
    "clf = KNN(contamination = outlier_fraction)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# You can print this to see all the prediction scores\n",
    "scores_pred = clf.decision_function(X_train)*-1\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "n_errors = (y_pred != y_train).sum()\n",
    "# Counting the number of errors\n",
    "\n",
    "print('The number of prediction errors are ' + str(n_errors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Visualising the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold value to consider a\n",
    "# datapoint inlier or outlier\n",
    "threshold = stats.scoreatpercentile(scores_pred, 100 * outlier_fraction)\n",
    "\n",
    "# decision function calculates the raw\n",
    "# anomaly score for every point\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) * -1\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# fill blue colormap from minimum anomaly\n",
    "# score to threshold value\n",
    "subplot = plt.subplot(1, 2, 1)\n",
    "subplot.contourf(xx, yy, Z, levels = np.linspace(Z.min(),\n",
    "\t\t\t\tthreshold, 10), cmap = plt.cm.Blues_r)\n",
    "\n",
    "# draw red contour line where anomaly\n",
    "# score is equal to threshold\n",
    "a = subplot.contour(xx, yy, Z, levels =[threshold],\n",
    "\t\t\t\t\tlinewidths = 2, colors ='red')\n",
    "\n",
    "# fill orange contour lines where range of anomaly\n",
    "# score is from threshold to maximum anomaly score\n",
    "subplot.contourf(xx, yy, Z, levels =[threshold, Z.max()], colors ='orange')\n",
    "\n",
    "# scatter plot of inliers with white dots\n",
    "b = subplot.scatter(X_train[:-n_outliers, 0], X_train[:-n_outliers, 1],\n",
    "\t\t\t\t\t\t\t\t\tc ='white', s = 20, edgecolor ='k')\n",
    "\n",
    "# scatter plot of outliers with black dots\n",
    "c = subplot.scatter(X_train[-n_outliers:, 0], X_train[-n_outliers:, 1],\n",
    "\t\t\t\t\t\t\t\t\tc ='black', s = 20, edgecolor ='k')\n",
    "subplot.axis('tight')\n",
    "\n",
    "subplot.legend(\n",
    "\t[a.collections[0], b, c],\n",
    "\t['learned decision function', 'true inliers', 'true outliers'],\n",
    "\tprop = matplotlib.font_manager.FontProperties(size = 10),\n",
    "\tloc ='lower right')\n",
    "\n",
    "subplot.set_title('K-Nearest Neighbours')\n",
    "subplot.set_xlim((-10, 10))\n",
    "subplot.set_ylim((-10, 10))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
